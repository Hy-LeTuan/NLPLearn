{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dataset import read_ner_file\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np \n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm \n",
    "\n",
    "from datasets import Dataset as trDataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "LABEL_2_ID = {'B-PATIENT_ID': 0, \n",
    "    'I-PATIENT_ID': 1, \n",
    "    'B-NAME': 2, \n",
    "    'I-NAME': 3, \n",
    "    'B-AGE': 4, \n",
    "    'I-AGE': 5, \n",
    "    'B-GENDER': 6, \n",
    "    'I-GENDER': 7, \n",
    "    'B-JOB': 8, \n",
    "    'I-JOB': 9, \n",
    "    'B-LOCATION': 10, \n",
    "    'I-LOCATION': 11, \n",
    "    'B-ORGANIZATION': 12, \n",
    "    'I-ORGANIZATION': 13, \n",
    "    'B-SYMPTOM_AND_DISEASE': 14, \n",
    "    'I-SYMPTOM_AND_DISEASE': 15, \n",
    "    'B-TRANSPORTATION': 16, \n",
    "    'I-TRANSPORTATION': 17, \n",
    "    'B-DATE': 18, \n",
    "    'I-DATE': 19, \n",
    "    'O': 20\n",
    "}\n",
    "\n",
    "ID_2_LABEL = {0: 'B-PATIENT_ID', \n",
    "    1: 'I-PATIENT_ID', \n",
    "    2: 'B-NAME', \n",
    "    3: 'I-NAME', \n",
    "    4: 'B-AGE', \n",
    "    5: 'I-AGE', \n",
    "    6: 'B-GENDER', \n",
    "    7: 'I-GENDER', \n",
    "    8: 'B-JOB', \n",
    "    9: 'I-JOB', \n",
    "    10: 'B-LOCATION', \n",
    "    11: 'I-LOCATION', \n",
    "    12: 'B-ORGANIZATION', \n",
    "    13: 'I-ORGANIZATION', \n",
    "    14: 'B-SYMPTOM_AND_DISEASE', \n",
    "    15: 'I-SYMPTOM_AND_DISEASE', \n",
    "    16: 'B-TRANSPORTATION', \n",
    "    17: 'I-TRANSPORTATION', \n",
    "    18: 'B-DATE', \n",
    "    19: 'I-DATE', \n",
    "    20: 'O'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = read_ner_file(\"./data/syllable/train_syllable.conll\")\n",
    "df_test = read_ner_file(\"./data/syllable/test_syllable.conll\")\n",
    "df_eval = read_ner_file(\"./data/syllable/dev_syllable.conll\")\n",
    "\n",
    "df_train = pd.DataFrame(data=df_train)\n",
    "df_train = df_train.convert_dtypes()\n",
    "\n",
    "df_test = pd.DataFrame(data=df_test) \n",
    "df_test = df_test.convert_dtypes()\n",
    "\n",
    "df_eval = pd.DataFrame(data=df_eval)\n",
    "df_eval = df_eval.convert_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-PATIENT_ID\n",
      "Token type: B-PATIENT_ID has 1960 occurences\n",
      "I-PATIENT_ID\n",
      "Token type: I-PATIENT_ID has 6 occurences\n",
      "B-NAME\n",
      "Token type: B-NAME has 288 occurences\n",
      "I-NAME\n",
      "Token type: I-NAME has 44 occurences\n",
      "B-AGE\n",
      "Token type: B-AGE has 611 occurences\n",
      "I-AGE\n",
      "Token type: I-AGE has 2 occurences\n",
      "B-GENDER\n",
      "Token type: B-GENDER has 503 occurences\n",
      "I-GENDER\n",
      "Token type: I-GENDER has 13 occurences\n",
      "B-JOB\n",
      "Token type: B-JOB has 196 occurences\n",
      "I-JOB\n",
      "Token type: I-JOB has 194 occurences\n",
      "B-LOCATION\n",
      "Token type: B-LOCATION has 2926 occurences\n",
      "I-LOCATION\n",
      "Token type: I-LOCATION has 2851 occurences\n",
      "B-ORGANIZATION\n",
      "Token type: B-ORGANIZATION has 983 occurences\n",
      "I-ORGANIZATION\n",
      "Token type: I-ORGANIZATION has 974 occurences\n",
      "B-SYMPTOM_AND_DISEASE\n",
      "Token type: B-SYMPTOM_AND_DISEASE has 618 occurences\n",
      "I-SYMPTOM_AND_DISEASE\n",
      "Token type: I-SYMPTOM_AND_DISEASE has 536 occurences\n",
      "B-TRANSPORTATION\n",
      "Token type: B-TRANSPORTATION has 213 occurences\n",
      "I-TRANSPORTATION\n",
      "Token type: I-TRANSPORTATION has 54 occurences\n",
      "B-DATE\n",
      "Token type: B-DATE has 2038 occurences\n",
      "I-DATE\n",
      "Token type: I-DATE has 1038 occurences\n",
      "O\n",
      "Token type: O has 5028 occurences\n",
      "21076\n"
     ]
    }
   ],
   "source": [
    "tokens = df_train[\"tokens\"]\n",
    "\n",
    "def get_token_type_count(tokens: pd.Series, classname): \n",
    "    tokens = tokens.apply(func=lambda x: True if classname in x else False)\n",
    "    pos = tokens[tokens == True].count()\n",
    "    return pos \n",
    "\n",
    "total = 0\n",
    "\n",
    "for key in LABEL_2_ID.keys(): \n",
    "    print(key)\n",
    "    count = get_token_type_count(tokens=tokens, classname=key)\n",
    "    total += count\n",
    "    print(f\"Token type: {key} has {count} occurences\")\n",
    "\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_dir = \"/home/hyle/Documents/vscode/NLPDataCollection/NLPDataCollection/tokenizer/trained_tokenizer/tokenizer-50k\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = trDataset.from_pandas(df_train, split=\"train\")\n",
    "test_dataset = trDataset.from_pandas(df_test, split=\"test\")\n",
    "val_dataset = trDataset.from_pandas(df_eval, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.model_max_length = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tokens(sample, tokenizer):\n",
    "    text = sample[\"words\"]\n",
    "    res = tokenizer(text, truncation=False, is_split_into_words=True)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=16): 100%|██████████| 5028/5028 [00:04<00:00, 1220.70 examples/s]\n",
      "Map (num_proc=16): 100%|██████████| 3000/3000 [00:02<00:00, 1269.73 examples/s]\n",
      "Map (num_proc=16): 100%|██████████| 2000/2000 [00:02<00:00, 932.88 examples/s] \n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(convert_to_tokens, batched=True, fn_kwargs={\"tokenizer\": tokenizer}, num_proc=os.cpu_count())\n",
    "test_dataset = test_dataset.map(convert_to_tokens, batched=True, fn_kwargs={\"tokenizer\": tokenizer}, num_proc=os.cpu_count())\n",
    "val_dataset = val_dataset.map(convert_to_tokens, batched=True, fn_kwargs={\"tokenizer\": tokenizer}, num_proc=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.remove_columns([\"words\", \"tokens\"])\n",
    "test_dataset = test_dataset.remove_columns([\"words\", \"tokens\"])\n",
    "val_dataset = val_dataset.remove_columns([\"words\", \"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df_train = train_dataset.to_pandas()\n",
    "save_df_test = test_dataset.to_pandas()\n",
    "save_df_val = val_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (0/1 shards):   0%|          | 0/5028 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 5028/5028 [00:00<00:00, 867822.74 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 3000/3000 [00:00<00:00, 563674.77 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 2000/2000 [00:00<00:00, 578884.00 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = trDataset.from_pandas(save_df_train).save_to_disk(\"./data/tokenized_dataset_train\")\n",
    "test_dataset = trDataset.from_pandas(save_df_test).save_to_disk(\"./data/tokenized_dataset_test\")\n",
    "val_dataset = trDataset.from_pandas(save_df_val).save_to_disk(\"./data/tokenized_dataset_val\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
