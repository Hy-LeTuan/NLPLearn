{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dataset import read_ner_file\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np \n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm \n",
    "import torcheval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_2_ID = {'B-PATIENT_ID': 0, \n",
    "    'I-PATIENT_ID': 1, \n",
    "    'B-NAME': 2, \n",
    "    'I-NAME': 3, \n",
    "    'B-AGE': 4, \n",
    "    'I-AGE': 5, \n",
    "    'B-GENDER': 6, \n",
    "    'I-GENDER': 7, \n",
    "    'B-JOB': 8, \n",
    "    'I-JOB': 9, \n",
    "    'B-LOCATION': 10, \n",
    "    'I-LOCATION': 11, \n",
    "    'B-ORGANIZATION': 12, \n",
    "    'I-ORGANIZATION': 13, \n",
    "    'B-SYMPTOM_AND_DISEASE': 14, \n",
    "    'I-SYMPTOM_AND_DISEASE': 15, \n",
    "    'B-TRANSPORTATION': 16, \n",
    "    'I-TRANSPORTATION': 17, \n",
    "    'B-DATE': 18, \n",
    "    'I-DATE': 19, \n",
    "    'O': 20\n",
    "}\n",
    "\n",
    "ID_2_LABEL = {0: 'B-PATIENT_ID', \n",
    "    1: 'I-PATIENT_ID', \n",
    "    2: 'B-NAME', \n",
    "    3: 'I-NAME', \n",
    "    4: 'B-AGE', \n",
    "    5: 'I-AGE', \n",
    "    6: 'B-GENDER', \n",
    "    7: 'I-GENDER', \n",
    "    8: 'B-JOB', \n",
    "    9: 'I-JOB', \n",
    "    10: 'B-LOCATION', \n",
    "    11: 'I-LOCATION', \n",
    "    12: 'B-ORGANIZATION', \n",
    "    13: 'I-ORGANIZATION', \n",
    "    14: 'B-SYMPTOM_AND_DISEASE', \n",
    "    15: 'I-SYMPTOM_AND_DISEASE', \n",
    "    16: 'B-TRANSPORTATION', \n",
    "    17: 'I-TRANSPORTATION', \n",
    "    18: 'B-DATE', \n",
    "    19: 'I-DATE', \n",
    "    20: 'O'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = read_ner_file(\"./data/syllable/train_syllable.conll\")\n",
    "# df_test = read_ner_file(\"./data/syllable/test_syllable.conll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(data=df_train)\n",
    "df_train = df_train.convert_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Đồng, thời, ,, bệnh, viện, tiếp, tục, thực, h...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[\", Số, bệnh, viện, có, thể, tiếp, nhận, bệnh,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-SYMPTOM_AN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Ngoài, ra, ,, những, người, tiếp, xúc, gián, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Bà, này, khi, trở, về, quá, cảnh, Doha, (, Qa...</td>\n",
       "      <td>[O, O, O, O, O, O, O, B-LOCATION, O, B-LOCATIO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[\", Bệnh, nhân, 523, \", và, chồng, là, \", bệnh...</td>\n",
       "      <td>[O, O, O, B-PATIENT_ID, O, O, O, O, O, O, O, B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5023</th>\n",
       "      <td>[Liên, quan, đến, Bệnh, viện, Bạch, Mai, ,, ôn...</td>\n",
       "      <td>[O, O, O, B-LOCATION, I-LOCATION, I-LOCATION, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5024</th>\n",
       "      <td>[Mẫu, lần, hai, ngày, 22/7, kết, quả, sàng, lọ...</td>\n",
       "      <td>[O, O, O, O, B-DATE, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5025</th>\n",
       "      <td>[Đây, là, 5, trường, hợp, dương, tính, được, B...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, B-ORGANIZATION, I-ORG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5026</th>\n",
       "      <td>[Lúc, 17h, ngày, 7, -, 3, ,, Viện, Vệ, sinh, D...</td>\n",
       "      <td>[O, O, O, B-DATE, I-DATE, I-DATE, O, B-ORGANIZ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5027</th>\n",
       "      <td>[Ngày, 12/8, ,, anh, được, cách, ly, tập, trun...</td>\n",
       "      <td>[O, B-DATE, O, O, O, O, O, O, O, O, O, B-DATE,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5028 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  words  \\\n",
       "0     [Đồng, thời, ,, bệnh, viện, tiếp, tục, thực, h...   \n",
       "1     [\", Số, bệnh, viện, có, thể, tiếp, nhận, bệnh,...   \n",
       "2     [Ngoài, ra, ,, những, người, tiếp, xúc, gián, ...   \n",
       "3     [Bà, này, khi, trở, về, quá, cảnh, Doha, (, Qa...   \n",
       "4     [\", Bệnh, nhân, 523, \", và, chồng, là, \", bệnh...   \n",
       "...                                                 ...   \n",
       "5023  [Liên, quan, đến, Bệnh, viện, Bạch, Mai, ,, ôn...   \n",
       "5024  [Mẫu, lần, hai, ngày, 22/7, kết, quả, sàng, lọ...   \n",
       "5025  [Đây, là, 5, trường, hợp, dương, tính, được, B...   \n",
       "5026  [Lúc, 17h, ngày, 7, -, 3, ,, Viện, Vệ, sinh, D...   \n",
       "5027  [Ngày, 12/8, ,, anh, được, cách, ly, tập, trun...   \n",
       "\n",
       "                                                 tokens  \n",
       "0     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "1     [O, O, O, O, O, O, O, O, O, O, O, B-SYMPTOM_AN...  \n",
       "2     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "3     [O, O, O, O, O, O, O, B-LOCATION, O, B-LOCATIO...  \n",
       "4     [O, O, O, B-PATIENT_ID, O, O, O, O, O, O, O, B...  \n",
       "...                                                 ...  \n",
       "5023  [O, O, O, B-LOCATION, I-LOCATION, I-LOCATION, ...  \n",
       "5024          [O, O, O, O, B-DATE, O, O, O, O, O, O, O]  \n",
       "5025  [O, O, O, O, O, O, O, O, B-ORGANIZATION, I-ORG...  \n",
       "5026  [O, O, O, B-DATE, I-DATE, I-DATE, O, B-ORGANIZ...  \n",
       "5027  [O, B-DATE, O, O, O, O, O, O, O, O, O, B-DATE,...  \n",
       "\n",
       "[5028 rows x 2 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'O']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "a = df_train[\"tokens\"].loc[0]\n",
    "print(a)\n",
    "print(type(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-PATIENT_ID\n",
      "Token type: B-PATIENT_ID has 1960 occurences\n",
      "I-PATIENT_ID\n",
      "Token type: I-PATIENT_ID has 6 occurences\n",
      "B-NAME\n",
      "Token type: B-NAME has 288 occurences\n",
      "I-NAME\n",
      "Token type: I-NAME has 44 occurences\n",
      "B-AGE\n",
      "Token type: B-AGE has 611 occurences\n",
      "I-AGE\n",
      "Token type: I-AGE has 2 occurences\n",
      "B-GENDER\n",
      "Token type: B-GENDER has 503 occurences\n",
      "I-GENDER\n",
      "Token type: I-GENDER has 13 occurences\n",
      "B-JOB\n",
      "Token type: B-JOB has 196 occurences\n",
      "I-JOB\n",
      "Token type: I-JOB has 194 occurences\n",
      "B-LOCATION\n",
      "Token type: B-LOCATION has 2926 occurences\n",
      "I-LOCATION\n",
      "Token type: I-LOCATION has 2851 occurences\n",
      "B-ORGANIZATION\n",
      "Token type: B-ORGANIZATION has 983 occurences\n",
      "I-ORGANIZATION\n",
      "Token type: I-ORGANIZATION has 974 occurences\n",
      "B-SYMPTOM_AND_DISEASE\n",
      "Token type: B-SYMPTOM_AND_DISEASE has 618 occurences\n",
      "I-SYMPTOM_AND_DISEASE\n",
      "Token type: I-SYMPTOM_AND_DISEASE has 536 occurences\n",
      "B-TRANSPORTATION\n",
      "Token type: B-TRANSPORTATION has 213 occurences\n",
      "I-TRANSPORTATION\n",
      "Token type: I-TRANSPORTATION has 54 occurences\n",
      "B-DATE\n",
      "Token type: B-DATE has 2038 occurences\n",
      "I-DATE\n",
      "Token type: I-DATE has 1038 occurences\n",
      "O\n",
      "Token type: O has 5028 occurences\n"
     ]
    }
   ],
   "source": [
    "tokens = df_train[\"tokens\"]\n",
    "\n",
    "def get_token_type_count(tokens: pd.Series, classname): \n",
    "    tokens = tokens.apply(func=lambda x: True if classname in x else False)\n",
    "    pos = tokens[tokens == True].count()\n",
    "    return pos \n",
    "\n",
    "\n",
    "for key in LABEL_2_ID.keys(): \n",
    "    print(key)\n",
    "    print(f\"Token type: {key} has {get_token_type_count(tokens=tokens, classname=key)} occurences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"uitnlp/visobert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaForMaskedLM(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(15004, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): XLMRobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=15004, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lm_head.decoder = nn.Linear(in_features=768, out_features=len(ID_2_LABEL), bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLMRobertaForMaskedLM(\n",
      "  (roberta): XLMRobertaModel(\n",
      "    (embeddings): XLMRobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(15004, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): XLMRobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x XLMRobertaLayer(\n",
      "          (attention): XLMRobertaAttention(\n",
      "            (self): XLMRobertaSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): XLMRobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): XLMRobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): XLMRobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): XLMRobertaLMHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (decoder): Linear(in_features=768, out_features=21, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for params in model.base_model.parameters(): \n",
    "    params.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"uitnlp/visobert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLMRobertaTokenizerFast(name_or_path='uitnlp/visobert', vocab_size=15002, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t15001: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converter(tokens): \n",
    "    converted_tokens = [] \n",
    "\n",
    "    for token in tokens: \n",
    "        converted_tokens.append(LABEL_2_ID[token])\n",
    "\n",
    "    return converted_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5028"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"tokens\"] = df_train[\"tokens\"].apply(func=converter)\n",
    "df_train[\"tokens\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 2...\n",
       "1       [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 1...\n",
       "2       [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 2...\n",
       "3       [20, 20, 20, 20, 20, 20, 20, 10, 20, 10, 20, 2...\n",
       "4       [20, 20, 20, 0, 20, 20, 20, 20, 20, 20, 20, 0,...\n",
       "                              ...                        \n",
       "5023    [20, 20, 20, 10, 11, 11, 11, 20, 20, 20, 20, 2...\n",
       "5024     [20, 20, 20, 20, 18, 20, 20, 20, 20, 20, 20, 20]\n",
       "5025    [20, 20, 20, 20, 20, 20, 20, 20, 12, 13, 13, 2...\n",
       "5026    [20, 20, 20, 18, 19, 19, 20, 12, 13, 13, 13, 1...\n",
       "5027    [20, 18, 20, 20, 20, 20, 20, 20, 20, 20, 20, 1...\n",
       "Name: tokens, Length: 5028, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5028"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"words\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "train_tokens = tokenizer(df_train[\"words\"].to_list(), truncation=True, padding=True, return_tensors=\"pt\", is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "tensor([   0, 2155,  543,  171, 1045, 1287,  609, 1159,  798,  673,  236, 4175,\n",
      "        1696,  964, 1270,  694, 1045, 6296,  317, 4963,  527, 1299, 1134,  104,\n",
      "        1718,  509,  980,  140,    2,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1])\n",
      "[20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 12, 13, 13, 20]\n",
      "[20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 12, 13, 13, 20]\n"
     ]
    }
   ],
   "source": [
    "word_ids = train_tokens.word_ids(batch_index=0)\n",
    "print(word_ids)\n",
    "print(train_tokens[\"input_ids\"][0])\n",
    "reconstruct = [] \n",
    "\n",
    "for word_idx in word_ids: \n",
    "    if word_idx != None: \n",
    "        reconstruct.append(df_train[\"tokens\"].loc[0][word_idx])\n",
    "\n",
    "print(reconstruct)\n",
    "print(df_train[\"tokens\"].loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5028, 241])\n",
      "torch.Size([5028, 241])\n",
      "241\n"
     ]
    }
   ],
   "source": [
    "print(train_tokens[\"input_ids\"].shape)\n",
    "print(train_tokens[\"attention_mask\"].shape) \n",
    "\n",
    "max_len = train_tokens[\"input_ids\"].shape[-1]\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_tokens(tokens, label_all_tokens=True): \n",
    "    labels = [] \n",
    "    for i, label in enumerate(df_train[\"tokens\"]):\n",
    "        word_ids = tokens.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokens[\"labels\"] = labels\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = align_tokens(train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5028\n"
     ]
    }
   ],
   "source": [
    "labels = train_tokens[\"labels\"]\n",
    "\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisoDataset(Dataset): \n",
    "    def __init__(self, tokens: pd.Series, labels: list): \n",
    "        self.labels = labels\n",
    "        self.input_ids = tokens[\"input_ids\"]\n",
    "        self.attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "        self.length = len(self.input_ids)\n",
    "\n",
    "    def __len__(self): \n",
    "        return self.length \n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        input_id = self.input_ids[idx]\n",
    "        attention_mask = self.attention_mask[idx]\n",
    "\n",
    "        return {\n",
    "            \"labels\": label, \n",
    "            \"input_ids\": input_id, \n",
    "            \"attention_mask\": attention_mask,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VisoDataset(tokens=train_tokens, labels=train_tokens[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([241])\n",
      "torch.Size([241])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0][\"labels\"].shape) \n",
    "print(train_dataset[1][\"labels\"].shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=128, shuffle=True, num_workers=16, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: tensor([[   0, 2589,  136,  ...,    1,    1,    1],\n",
      "        [   0,  483,   77,  ...,    1,    1,    1],\n",
      "        [   0, 1654,   29,  ...,    1,    1,    1],\n",
      "        ...,\n",
      "        [   0, 1272, 1213,  ...,    1,    1,    1],\n",
      "        [   0, 2367,  136,  ...,    1,    1,    1],\n",
      "        [   0, 1201, 2186,  ...,    1,    1,    1]])\n",
      "labels: tensor([[-100,   20,   20,  ..., -100, -100, -100],\n",
      "        [-100,   20,   20,  ..., -100, -100, -100],\n",
      "        [-100,   20,   20,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [-100,   20,   20,  ..., -100, -100, -100],\n",
      "        [-100,   20,   20,  ..., -100, -100, -100],\n",
      "        [-100,   20,   10,  ..., -100, -100, -100]])\n"
     ]
    }
   ],
   "source": [
    "for input_dict in train_loader: \n",
    "    print(f\"input_ids: {input_dict['input_ids']}\")\n",
    "    print(f\"labels: {input_dict['labels']}\")\n",
    "\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyle/miniconda3/envs/dl/lib/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = nn.CrossEntropyLoss(reduction=\"mean\", ignore_index=-100)\n",
    "\n",
    "output_metrics = {\n",
    "    \"train\": {\n",
    "        \"loss\": [], \n",
    "        \"accuracy\": [], \n",
    "        \"precision\": [], \n",
    "        \"recall\": [], \n",
    "        \"f1\": [], \n",
    "    }, \n",
    "    \"eval\": {\n",
    "        \"loss\": [], \n",
    "        \"accuracy\": [], \n",
    "        \"precision\": [], \n",
    "        \"recall\": [], \n",
    "        \"f1\": [], \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 3/3 [07:36<00:00, 152.17s/it]\n"
     ]
    }
   ],
   "source": [
    "import torcheval.metrics\n",
    "\n",
    "\n",
    "epochs = 15\n",
    "\n",
    "for i in tqdm(range(epochs), desc=\"Epochs\", total=epochs): \n",
    "\n",
    "    # training loop\n",
    "    epoch_loss = 0.0 \n",
    "    epoch_accuracy = 0.0\n",
    "    epoch_precision = 0.0\n",
    "    epoch_recall = 0.0\n",
    "    epoch_f1 = 0.0\n",
    "    total_steps = len(train_loader)\n",
    "    model = model.train()\n",
    "\n",
    "    for input_dict in tqdm(train_loader, desc=f\"Training batches epoch {i}\", total=total_steps):\n",
    "        input_ids = input_dict[\"input_ids\"].to(device)\n",
    "        labels = input_dict[\"labels\"].to(device)\n",
    "        attention_mask = input_dict[\"attention_mask\"].to(device)\n",
    "\n",
    "\n",
    "        logits = model(input_ids, attention_mask)[\"logits\"]\n",
    "\n",
    "        labels = labels.view(-1)\n",
    "        logits = logits.view(-1, 21) \n",
    "\n",
    "        # compute loss \n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        # backpropagation \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update running loss\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # compute metrics\n",
    "        with torch.no_grad(): \n",
    "            epoch_accuracy += torcheval.metrics.functional.multiclass_accuracy(input=logits, target=labels, average=\"micro\").item()\n",
    "            epoch_f1 += torcheval.metrics.functional.multiclass_f1_score(input=logits, target=labels, average=\"micro\").item()\n",
    "            epoch_precision += torcheval.metrics.functional.multiclass_precision(input=logits, target=labels, average=\"micro\").item()\n",
    "            epoch_recall += torcheval.metrics.functional.multiclass_recall(input=logits, target=labels, average=\"micro\").item()\n",
    "\n",
    "    output_metrics[\"train\"][\"loss\"].append(epoch_loss / total_steps)\n",
    "    output_metrics[\"train\"][\"accuracy\"].append(epoch_accuracy / total_steps)\n",
    "    output_metrics[\"train\"][\"precision\"].append(epoch_precision / total_steps)\n",
    "    output_metrics[\"train\"][\"recall\"].append(epoch_recall / total_steps)\n",
    "    output_metrics[\"train\"][\"f1\"].append(epoch_f1 / total_steps)\n",
    "\n",
    "    # evaluation loop\n",
    "    model = model.eval()\n",
    "    epoch_loss = 0.0 \n",
    "    epoch_accuracy = 0.0\n",
    "    epoch_precision = 0.0\n",
    "    epoch_recall = 0.0\n",
    "    epoch_f1 = 0.0\n",
    "    total_steps = len(train_loader)\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        for input_dict in tqdm(train_loader, desc=f\"Testing batches epoch {i}\", total=total_steps): \n",
    "            input_ids = input_dict[\"input_ids\"].to(device)\n",
    "            labels = input_dict[\"labels\"].to(device)\n",
    "            attention_mask = input_dict[\"attention_mask\"].to(device)\n",
    "\n",
    "\n",
    "            logits = model(input_ids, attention_mask)[\"logits\"]\n",
    "\n",
    "            labels = labels.view(-1)\n",
    "            logits = logits.view(-1, 21) \n",
    "\n",
    "            # compute loss \n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            # update running loss\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # compute metrics\n",
    "            epoch_accuracy += torcheval.metrics.functional.multiclass_accuracy(input=logits, target=labels, average=\"micro\").item()\n",
    "            epoch_f1 += torcheval.metrics.functional.multiclass_f1_score(input=logits, target=labels, average=\"micro\").item()\n",
    "            epoch_precision += torcheval.metrics.functional.multiclass_precision(input=logits, target=labels, average=\"micro\").item()\n",
    "            epoch_recall += torcheval.metrics.functional.multiclass_recall(input=logits, target=labels, average=\"micro\").item()\n",
    "\n",
    "    output_metrics[\"eval\"][\"loss\"].append(epoch_loss / total_steps)\n",
    "    output_metrics[\"eval\"][\"accuracy\"].append(epoch_accuracy / total_steps)\n",
    "    output_metrics[\"eval\"][\"precision\"].append(epoch_precision / total_steps)\n",
    "    output_metrics[\"eval\"][\"recall\"].append(epoch_recall / total_steps)\n",
    "    output_metrics[\"eval\"][\"f1\"].append(epoch_f1 / total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
