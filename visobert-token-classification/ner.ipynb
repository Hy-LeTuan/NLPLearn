{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dataset import read_ner_file\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np \n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_2_ID = {'B-PATIENT_ID': 0, \n",
    "    'I-PATIENT_ID': 1, \n",
    "    'B-NAME': 2, \n",
    "    'I-NAME': 3, \n",
    "    'B-AGE': 4, \n",
    "    'I-AGE': 5, \n",
    "    'B-GENDER': 6, \n",
    "    'I-GENDER': 7, \n",
    "    'B-JOB': 8, \n",
    "    'I-JOB': 9, \n",
    "    'B-LOCATION': 10, \n",
    "    'I-LOCATION': 11, \n",
    "    'B-ORGANIZATION': 12, \n",
    "    'I-ORGANIZATION': 13, \n",
    "    'B-SYMPTOM_AND_DISEASE': 14, \n",
    "    'I-SYMPTOM_AND_DISEASE': 15, \n",
    "    'B-TRANSPORTATION': 16, \n",
    "    'I-TRANSPORTATION': 17, \n",
    "    'B-DATE': 18, \n",
    "    'I-DATE': 19, \n",
    "    'O': 20\n",
    "}\n",
    "\n",
    "ID_2_LABEL = {0: 'B-PATIENT_ID', \n",
    "    1: 'I-PATIENT_ID', \n",
    "    2: 'B-NAME', \n",
    "    3: 'I-NAME', \n",
    "    4: 'B-AGE', \n",
    "    5: 'I-AGE', \n",
    "    6: 'B-GENDER', \n",
    "    7: 'I-GENDER', \n",
    "    8: 'B-JOB', \n",
    "    9: 'I-JOB', \n",
    "    10: 'B-LOCATION', \n",
    "    11: 'I-LOCATION', \n",
    "    12: 'B-ORGANIZATION', \n",
    "    13: 'I-ORGANIZATION', \n",
    "    14: 'B-SYMPTOM_AND_DISEASE', \n",
    "    15: 'I-SYMPTOM_AND_DISEASE', \n",
    "    16: 'B-TRANSPORTATION', \n",
    "    17: 'I-TRANSPORTATION', \n",
    "    18: 'B-DATE', \n",
    "    19: 'I-DATE', \n",
    "    20: 'O'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = read_ner_file(\"./data/syllable/train_syllable.conll\")\n",
    "# df_test = read_ner_file(\"./data/syllable/test_syllable.conll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(data=df_train)\n",
    "df_train = df_train.convert_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Đồng, thời, ,, bệnh, viện, tiếp, tục, thực, h...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[\", Số, bệnh, viện, có, thể, tiếp, nhận, bệnh,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-SYMPTOM_AN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Ngoài, ra, ,, những, người, tiếp, xúc, gián, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Bà, này, khi, trở, về, quá, cảnh, Doha, (, Qa...</td>\n",
       "      <td>[O, O, O, O, O, O, O, B-LOCATION, O, B-LOCATIO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[\", Bệnh, nhân, 523, \", và, chồng, là, \", bệnh...</td>\n",
       "      <td>[O, O, O, B-PATIENT_ID, O, O, O, O, O, O, O, B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5022</th>\n",
       "      <td>[Liên, quan, đến, Bệnh, viện, Bạch, Mai, ,, ôn...</td>\n",
       "      <td>[O, O, O, B-LOCATION, I-LOCATION, I-LOCATION, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5023</th>\n",
       "      <td>[Mẫu, lần, hai, ngày, 22/7, kết, quả, sàng, lọ...</td>\n",
       "      <td>[O, O, O, O, B-DATE, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5024</th>\n",
       "      <td>[Đây, là, 5, trường, hợp, dương, tính, được, B...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, B-ORGANIZATION, I-ORG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5025</th>\n",
       "      <td>[Lúc, 17h, ngày, 7, -, 3, ,, Viện, Vệ, sinh, D...</td>\n",
       "      <td>[O, O, O, B-DATE, I-DATE, I-DATE, O, B-ORGANIZ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5026</th>\n",
       "      <td>[Ngày, 12/8, ,, anh, được, cách, ly, tập, trun...</td>\n",
       "      <td>[O, B-DATE, O, O, O, O, O, O, O, O, O, B-DATE,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5027 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  words  \\\n",
       "0     [Đồng, thời, ,, bệnh, viện, tiếp, tục, thực, h...   \n",
       "1     [\", Số, bệnh, viện, có, thể, tiếp, nhận, bệnh,...   \n",
       "2     [Ngoài, ra, ,, những, người, tiếp, xúc, gián, ...   \n",
       "3     [Bà, này, khi, trở, về, quá, cảnh, Doha, (, Qa...   \n",
       "4     [\", Bệnh, nhân, 523, \", và, chồng, là, \", bệnh...   \n",
       "...                                                 ...   \n",
       "5022  [Liên, quan, đến, Bệnh, viện, Bạch, Mai, ,, ôn...   \n",
       "5023  [Mẫu, lần, hai, ngày, 22/7, kết, quả, sàng, lọ...   \n",
       "5024  [Đây, là, 5, trường, hợp, dương, tính, được, B...   \n",
       "5025  [Lúc, 17h, ngày, 7, -, 3, ,, Viện, Vệ, sinh, D...   \n",
       "5026  [Ngày, 12/8, ,, anh, được, cách, ly, tập, trun...   \n",
       "\n",
       "                                                 tokens  \n",
       "0     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "1     [O, O, O, O, O, O, O, O, O, O, O, B-SYMPTOM_AN...  \n",
       "2     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "3     [O, O, O, O, O, O, O, B-LOCATION, O, B-LOCATIO...  \n",
       "4     [O, O, O, B-PATIENT_ID, O, O, O, O, O, O, O, B...  \n",
       "...                                                 ...  \n",
       "5022  [O, O, O, B-LOCATION, I-LOCATION, I-LOCATION, ...  \n",
       "5023          [O, O, O, O, B-DATE, O, O, O, O, O, O, O]  \n",
       "5024  [O, O, O, O, O, O, O, O, B-ORGANIZATION, I-ORG...  \n",
       "5025  [O, O, O, B-DATE, I-DATE, I-DATE, O, B-ORGANIZ...  \n",
       "5026  [O, B-DATE, O, O, O, O, O, O, O, O, O, B-DATE,...  \n",
       "\n",
       "[5027 rows x 2 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'O']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "a = df_train[\"tokens\"].loc[0]\n",
    "print(a)\n",
    "print(type(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-PATIENT_ID\n",
      "Token type: B-PATIENT_ID has 1960 occurences\n",
      "I-PATIENT_ID\n",
      "Token type: I-PATIENT_ID has 6 occurences\n",
      "B-NAME\n",
      "Token type: B-NAME has 288 occurences\n",
      "I-NAME\n",
      "Token type: I-NAME has 44 occurences\n",
      "B-AGE\n",
      "Token type: B-AGE has 611 occurences\n",
      "I-AGE\n",
      "Token type: I-AGE has 2 occurences\n",
      "B-GENDER\n",
      "Token type: B-GENDER has 503 occurences\n",
      "I-GENDER\n",
      "Token type: I-GENDER has 13 occurences\n",
      "B-JOB\n",
      "Token type: B-JOB has 196 occurences\n",
      "I-JOB\n",
      "Token type: I-JOB has 194 occurences\n",
      "B-LOCATION\n",
      "Token type: B-LOCATION has 2926 occurences\n",
      "I-LOCATION\n",
      "Token type: I-LOCATION has 2851 occurences\n",
      "B-ORGANIZATION\n",
      "Token type: B-ORGANIZATION has 983 occurences\n",
      "I-ORGANIZATION\n",
      "Token type: I-ORGANIZATION has 974 occurences\n",
      "B-SYMPTOM_AND_DISEASE\n",
      "Token type: B-SYMPTOM_AND_DISEASE has 617 occurences\n",
      "I-SYMPTOM_AND_DISEASE\n",
      "Token type: I-SYMPTOM_AND_DISEASE has 535 occurences\n",
      "B-TRANSPORTATION\n",
      "Token type: B-TRANSPORTATION has 213 occurences\n",
      "I-TRANSPORTATION\n",
      "Token type: I-TRANSPORTATION has 54 occurences\n",
      "B-DATE\n",
      "Token type: B-DATE has 2038 occurences\n",
      "I-DATE\n",
      "Token type: I-DATE has 1038 occurences\n",
      "O\n",
      "Token type: O has 5027 occurences\n"
     ]
    }
   ],
   "source": [
    "tokens = df_train[\"tokens\"]\n",
    "\n",
    "def get_token_type_count(tokens: pd.Series, classname): \n",
    "    tokens = tokens.apply(func=lambda x: True if classname in x else False)\n",
    "    pos = tokens[tokens == True].count()\n",
    "    return pos \n",
    "\n",
    "\n",
    "for key in LABEL_2_ID.keys(): \n",
    "    print(key)\n",
    "    print(f\"Token type: {key} has {get_token_type_count(tokens=tokens, classname=key)} occurences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"uitnlp/visobert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaForMaskedLM(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(15004, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): XLMRobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=15004, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lm_head.decoder = nn.Linear(in_features=768, out_features=len(ID_2_LABEL), bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLMRobertaForMaskedLM(\n",
      "  (roberta): XLMRobertaModel(\n",
      "    (embeddings): XLMRobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(15004, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): XLMRobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x XLMRobertaLayer(\n",
      "          (attention): XLMRobertaAttention(\n",
      "            (self): XLMRobertaSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): XLMRobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): XLMRobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): XLMRobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): XLMRobertaLMHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (decoder): Linear(in_features=768, out_features=21, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "for params in model.base_model.parameters(): \n",
    "    params.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"uitnlp/visobert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLMRobertaTokenizerFast(name_or_path='uitnlp/visobert', vocab_size=15002, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t15001: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converter(tokens): \n",
    "    converted_tokens = [] \n",
    "\n",
    "    for token in tokens: \n",
    "        converted_tokens.append(LABEL_2_ID[token])\n",
    "\n",
    "    return converted_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5027"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"tokens\"] = df_train[\"tokens\"].apply(func=converter)\n",
    "df_train[\"tokens\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 2...\n",
       "1       [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 1...\n",
       "2       [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 2...\n",
       "3       [20, 20, 20, 20, 20, 20, 20, 10, 20, 10, 20, 2...\n",
       "4       [20, 20, 20, 0, 20, 20, 20, 20, 20, 20, 20, 0,...\n",
       "                              ...                        \n",
       "5022    [20, 20, 20, 10, 11, 11, 11, 20, 20, 20, 20, 2...\n",
       "5023     [20, 20, 20, 20, 18, 20, 20, 20, 20, 20, 20, 20]\n",
       "5024    [20, 20, 20, 20, 20, 20, 20, 20, 12, 13, 13, 2...\n",
       "5025    [20, 20, 20, 18, 19, 19, 20, 12, 13, 13, 13, 1...\n",
       "5026    [20, 18, 20, 20, 20, 20, 20, 20, 20, 20, 20, 1...\n",
       "Name: tokens, Length: 5027, dtype: object"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5027"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"words\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "train_tokens = tokenizer(df_train[\"words\"].to_list(), truncation=True, padding=True, return_tensors=\"pt\", is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5027, 241])\n",
      "torch.Size([5027, 241])\n",
      "241\n"
     ]
    }
   ],
   "source": [
    "print(train_tokens[\"input_ids\"].shape)\n",
    "print(train_tokens[\"attention_mask\"].shape) \n",
    "\n",
    "max_len = train_tokens[\"input_ids\"].shape[-1]\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(tokens, max_len): \n",
    "    needed = [-100] * (max_len - len(tokens))\n",
    "    tokens = tokens + needed\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"tokens\"] = df_train[\"tokens\"].apply(func=pad, args=(max_len,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241\n",
      "241\n"
     ]
    }
   ],
   "source": [
    "print(len(df_train[\"tokens\"].loc[0]))\n",
    "print(len(df_train[\"tokens\"].loc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisoDataset(Dataset): \n",
    "    def __init__(self, tokens: pd.Series, label: pd.Series): \n",
    "        self.label = label\n",
    "        self.input_ids = tokens[\"input_ids\"]\n",
    "        self.attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "        self.length = len(self.input_ids)\n",
    "\n",
    "    def __len__(self): \n",
    "        return self.length \n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        label = torch.tensor(self.label.loc[idx], dtype=torch.long)\n",
    "        label = label.view(label.shape[0], 1)\n",
    "        input_id = self.input_ids[idx]\n",
    "        attention_mask = self.attention_mask[idx]\n",
    "\n",
    "        return {\n",
    "            \"labels\": label, \n",
    "            \"input_ids\": input_id, \n",
    "            \"attention_mask\": attention_mask,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VisoDataset(tokens=train_tokens, label=df_train[\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([241, 1])\n",
      "torch.Size([241, 1])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0][\"labels\"].shape) \n",
    "print(train_dataset[1][\"labels\"].shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=128, shuffle=True, num_workers=16, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyle/miniconda3/envs/dl/lib/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = nn.CrossEntropyLoss(reduction=\"mean\", ignore_index=-100)\n",
    "\n",
    "loss_history = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 10/10 [12:32<00:00, 75.25s/it]\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for i in tqdm(range(epochs), desc=\"Epochs\", total=epochs): \n",
    "    epoch_loss = 0.0 \n",
    "    total_steps = len(train_loader)\n",
    "\n",
    "    for input_dict in train_loader: \n",
    "        input_ids = input_dict[\"input_ids\"].to(device)\n",
    "        labels = input_dict[\"labels\"].to(device)\n",
    "        attention_mask = input_dict[\"attention_mask\"].to(device)\n",
    "\n",
    "        logits = model(input_ids, attention_mask)[\"logits\"]\n",
    "\n",
    "        loss = criterion(logits.view(-1, 21), labels.view(-1))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    loss_history.append(epoch_loss / total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.1813471496105192,\n",
       " 1.5255949735641479,\n",
       " 1.5425218373537064,\n",
       " 1.2903192818164826,\n",
       " 1.2050439774990083,\n",
       " 1.3115523397922515,\n",
       " 1.2227326959371567,\n",
       " 1.0553439140319825,\n",
       " 1.1479853570461274,\n",
       " 1.2268339544534683]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
