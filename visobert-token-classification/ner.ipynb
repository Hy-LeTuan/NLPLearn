{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dataset import read_ner_file\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np \n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm \n",
    "import torcheval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "LABEL_2_ID = {'B-PATIENT_ID': 0, \n",
    "    'I-PATIENT_ID': 1, \n",
    "    'B-NAME': 2, \n",
    "    'I-NAME': 3, \n",
    "    'B-AGE': 4, \n",
    "    'I-AGE': 5, \n",
    "    'B-GENDER': 6, \n",
    "    'I-GENDER': 7, \n",
    "    'B-JOB': 8, \n",
    "    'I-JOB': 9, \n",
    "    'B-LOCATION': 10, \n",
    "    'I-LOCATION': 11, \n",
    "    'B-ORGANIZATION': 12, \n",
    "    'I-ORGANIZATION': 13, \n",
    "    'B-SYMPTOM_AND_DISEASE': 14, \n",
    "    'I-SYMPTOM_AND_DISEASE': 15, \n",
    "    'B-TRANSPORTATION': 16, \n",
    "    'I-TRANSPORTATION': 17, \n",
    "    'B-DATE': 18, \n",
    "    'I-DATE': 19, \n",
    "    'O': 20\n",
    "}\n",
    "\n",
    "ID_2_LABEL = {0: 'B-PATIENT_ID', \n",
    "    1: 'I-PATIENT_ID', \n",
    "    2: 'B-NAME', \n",
    "    3: 'I-NAME', \n",
    "    4: 'B-AGE', \n",
    "    5: 'I-AGE', \n",
    "    6: 'B-GENDER', \n",
    "    7: 'I-GENDER', \n",
    "    8: 'B-JOB', \n",
    "    9: 'I-JOB', \n",
    "    10: 'B-LOCATION', \n",
    "    11: 'I-LOCATION', \n",
    "    12: 'B-ORGANIZATION', \n",
    "    13: 'I-ORGANIZATION', \n",
    "    14: 'B-SYMPTOM_AND_DISEASE', \n",
    "    15: 'I-SYMPTOM_AND_DISEASE', \n",
    "    16: 'B-TRANSPORTATION', \n",
    "    17: 'I-TRANSPORTATION', \n",
    "    18: 'B-DATE', \n",
    "    19: 'I-DATE', \n",
    "    20: 'O'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = read_ner_file(\"./data/syllable/train_syllable.conll\")\n",
    "df_test = read_ner_file(\"./data/syllable/test_syllable.conll\")\n",
    "\n",
    "df_train = pd.DataFrame(data=df_train)\n",
    "df_train = df_train.convert_dtypes()\n",
    "\n",
    "df_test = pd.DataFrame(data=df_test) \n",
    "df_test = df_test.convert_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Đồng, thời, ,, bệnh, viện, tiếp, tục, thực, h...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[\", Số, bệnh, viện, có, thể, tiếp, nhận, bệnh,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-SYMPTOM_AN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Ngoài, ra, ,, những, người, tiếp, xúc, gián, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Bà, này, khi, trở, về, quá, cảnh, Doha, (, Qa...</td>\n",
       "      <td>[O, O, O, O, O, O, O, B-LOCATION, O, B-LOCATIO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[\", Bệnh, nhân, 523, \", và, chồng, là, \", bệnh...</td>\n",
       "      <td>[O, O, O, B-PATIENT_ID, O, O, O, O, O, O, O, B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5023</th>\n",
       "      <td>[Liên, quan, đến, Bệnh, viện, Bạch, Mai, ,, ôn...</td>\n",
       "      <td>[O, O, O, B-LOCATION, I-LOCATION, I-LOCATION, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5024</th>\n",
       "      <td>[Mẫu, lần, hai, ngày, 22/7, kết, quả, sàng, lọ...</td>\n",
       "      <td>[O, O, O, O, B-DATE, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5025</th>\n",
       "      <td>[Đây, là, 5, trường, hợp, dương, tính, được, B...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, B-ORGANIZATION, I-ORG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5026</th>\n",
       "      <td>[Lúc, 17h, ngày, 7, -, 3, ,, Viện, Vệ, sinh, D...</td>\n",
       "      <td>[O, O, O, B-DATE, I-DATE, I-DATE, O, B-ORGANIZ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5027</th>\n",
       "      <td>[Ngày, 12/8, ,, anh, được, cách, ly, tập, trun...</td>\n",
       "      <td>[O, B-DATE, O, O, O, O, O, O, O, O, O, B-DATE,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5028 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  words  \\\n",
       "0     [Đồng, thời, ,, bệnh, viện, tiếp, tục, thực, h...   \n",
       "1     [\", Số, bệnh, viện, có, thể, tiếp, nhận, bệnh,...   \n",
       "2     [Ngoài, ra, ,, những, người, tiếp, xúc, gián, ...   \n",
       "3     [Bà, này, khi, trở, về, quá, cảnh, Doha, (, Qa...   \n",
       "4     [\", Bệnh, nhân, 523, \", và, chồng, là, \", bệnh...   \n",
       "...                                                 ...   \n",
       "5023  [Liên, quan, đến, Bệnh, viện, Bạch, Mai, ,, ôn...   \n",
       "5024  [Mẫu, lần, hai, ngày, 22/7, kết, quả, sàng, lọ...   \n",
       "5025  [Đây, là, 5, trường, hợp, dương, tính, được, B...   \n",
       "5026  [Lúc, 17h, ngày, 7, -, 3, ,, Viện, Vệ, sinh, D...   \n",
       "5027  [Ngày, 12/8, ,, anh, được, cách, ly, tập, trun...   \n",
       "\n",
       "                                                 tokens  \n",
       "0     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "1     [O, O, O, O, O, O, O, O, O, O, O, B-SYMPTOM_AN...  \n",
       "2     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "3     [O, O, O, O, O, O, O, B-LOCATION, O, B-LOCATIO...  \n",
       "4     [O, O, O, B-PATIENT_ID, O, O, O, O, O, O, O, B...  \n",
       "...                                                 ...  \n",
       "5023  [O, O, O, B-LOCATION, I-LOCATION, I-LOCATION, ...  \n",
       "5024          [O, O, O, O, B-DATE, O, O, O, O, O, O, O]  \n",
       "5025  [O, O, O, O, O, O, O, O, B-ORGANIZATION, I-ORG...  \n",
       "5026  [O, O, O, B-DATE, I-DATE, I-DATE, O, B-ORGANIZ...  \n",
       "5027  [O, B-DATE, O, O, O, O, O, O, O, O, O, B-DATE,...  \n",
       "\n",
       "[5028 rows x 2 columns]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Từ, 24, -, 7, đến, 31, -, 7, ,, bệnh, nhân, đ...</td>\n",
       "      <td>[O, B-DATE, I-DATE, I-DATE, O, B-DATE, I-DATE,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Bác, sĩ, Trần, Thanh, Linh, ,, từ, Bệnh, viện...</td>\n",
       "      <td>[O, O, O, O, O, O, O, B-ORGANIZATION, I-ORGANI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Theo, đó, ,, Sở, Y, tế, Bình, Thuận, cho, biế...</td>\n",
       "      <td>[O, O, O, B-ORGANIZATION, I-ORGANIZATION, I-OR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Bệnh, nhân, 218, :, nữ, ,, 43, tuổi, ,, quốc,...</td>\n",
       "      <td>[O, O, B-PATIENT_ID, O, B-GENDER, O, B-AGE, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Ông, cùng, 4, người, khác, hôm, 4/3, từ, Mala...</td>\n",
       "      <td>[O, O, O, O, O, O, B-DATE, O, B-LOCATION, O, B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>[Bệnh, nhân, điều, trị, tại, Bệnh, viện, Đa, k...</td>\n",
       "      <td>[O, O, O, O, O, B-LOCATION, I-LOCATION, I-LOCA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>[Hiện, ông, được, cách, ly, theo, dõi, tại, Tr...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, B-LOCATION, I-LOCATIO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>[Tính, đến, sáng, 19/3, ,, TP, HCM, ghi, nhận,...</td>\n",
       "      <td>[O, O, O, B-DATE, O, B-LOCATION, I-LOCATION, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>[Ngày, 20, -, 2, ,, cùng, lúc, có, 3, bệnh, nh...</td>\n",
       "      <td>[O, B-DATE, I-DATE, I-DATE, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>[Hiện, các, bệnh, nhân, đang, cách, ly, ,, điề...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-LOCATION, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  words  \\\n",
       "0     [Từ, 24, -, 7, đến, 31, -, 7, ,, bệnh, nhân, đ...   \n",
       "1     [Bác, sĩ, Trần, Thanh, Linh, ,, từ, Bệnh, viện...   \n",
       "2     [Theo, đó, ,, Sở, Y, tế, Bình, Thuận, cho, biế...   \n",
       "3     [Bệnh, nhân, 218, :, nữ, ,, 43, tuổi, ,, quốc,...   \n",
       "4     [Ông, cùng, 4, người, khác, hôm, 4/3, từ, Mala...   \n",
       "...                                                 ...   \n",
       "2995  [Bệnh, nhân, điều, trị, tại, Bệnh, viện, Đa, k...   \n",
       "2996  [Hiện, ông, được, cách, ly, theo, dõi, tại, Tr...   \n",
       "2997  [Tính, đến, sáng, 19/3, ,, TP, HCM, ghi, nhận,...   \n",
       "2998  [Ngày, 20, -, 2, ,, cùng, lúc, có, 3, bệnh, nh...   \n",
       "2999  [Hiện, các, bệnh, nhân, đang, cách, ly, ,, điề...   \n",
       "\n",
       "                                                 tokens  \n",
       "0     [O, B-DATE, I-DATE, I-DATE, O, B-DATE, I-DATE,...  \n",
       "1     [O, O, O, O, O, O, O, B-ORGANIZATION, I-ORGANI...  \n",
       "2     [O, O, O, B-ORGANIZATION, I-ORGANIZATION, I-OR...  \n",
       "3     [O, O, B-PATIENT_ID, O, B-GENDER, O, B-AGE, O,...  \n",
       "4     [O, O, O, O, O, O, B-DATE, O, B-LOCATION, O, B...  \n",
       "...                                                 ...  \n",
       "2995  [O, O, O, O, O, B-LOCATION, I-LOCATION, I-LOCA...  \n",
       "2996  [O, O, O, O, O, O, O, O, B-LOCATION, I-LOCATIO...  \n",
       "2997  [O, O, O, B-DATE, O, B-LOCATION, I-LOCATION, O...  \n",
       "2998  [O, B-DATE, I-DATE, I-DATE, O, O, O, O, O, O, ...  \n",
       "2999  [O, O, O, O, O, O, O, O, O, O, O, B-LOCATION, ...  \n",
       "\n",
       "[3000 rows x 2 columns]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-PATIENT_ID\n",
      "Token type: B-PATIENT_ID has 1960 occurences\n",
      "I-PATIENT_ID\n",
      "Token type: I-PATIENT_ID has 6 occurences\n",
      "B-NAME\n",
      "Token type: B-NAME has 288 occurences\n",
      "I-NAME\n",
      "Token type: I-NAME has 44 occurences\n",
      "B-AGE\n",
      "Token type: B-AGE has 611 occurences\n",
      "I-AGE\n",
      "Token type: I-AGE has 2 occurences\n",
      "B-GENDER\n",
      "Token type: B-GENDER has 503 occurences\n",
      "I-GENDER\n",
      "Token type: I-GENDER has 13 occurences\n",
      "B-JOB\n",
      "Token type: B-JOB has 196 occurences\n",
      "I-JOB\n",
      "Token type: I-JOB has 194 occurences\n",
      "B-LOCATION\n",
      "Token type: B-LOCATION has 2926 occurences\n",
      "I-LOCATION\n",
      "Token type: I-LOCATION has 2851 occurences\n",
      "B-ORGANIZATION\n",
      "Token type: B-ORGANIZATION has 983 occurences\n",
      "I-ORGANIZATION\n",
      "Token type: I-ORGANIZATION has 974 occurences\n",
      "B-SYMPTOM_AND_DISEASE\n",
      "Token type: B-SYMPTOM_AND_DISEASE has 618 occurences\n",
      "I-SYMPTOM_AND_DISEASE\n",
      "Token type: I-SYMPTOM_AND_DISEASE has 536 occurences\n",
      "B-TRANSPORTATION\n",
      "Token type: B-TRANSPORTATION has 213 occurences\n",
      "I-TRANSPORTATION\n",
      "Token type: I-TRANSPORTATION has 54 occurences\n",
      "B-DATE\n",
      "Token type: B-DATE has 2038 occurences\n",
      "I-DATE\n",
      "Token type: I-DATE has 1038 occurences\n",
      "O\n",
      "Token type: O has 5028 occurences\n"
     ]
    }
   ],
   "source": [
    "tokens = df_train[\"tokens\"]\n",
    "\n",
    "def get_token_type_count(tokens: pd.Series, classname): \n",
    "    tokens = tokens.apply(func=lambda x: True if classname in x else False)\n",
    "    pos = tokens[tokens == True].count()\n",
    "    return pos \n",
    "\n",
    "\n",
    "for key in LABEL_2_ID.keys(): \n",
    "    print(key)\n",
    "    print(f\"Token type: {key} has {get_token_type_count(tokens=tokens, classname=key)} occurences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"uitnlp/visobert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lm_head.decoder = nn.Linear(in_features=768, out_features=len(ID_2_LABEL), bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLMRobertaForMaskedLM(\n",
      "  (roberta): XLMRobertaModel(\n",
      "    (embeddings): XLMRobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(15004, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): XLMRobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x XLMRobertaLayer(\n",
      "          (attention): XLMRobertaAttention(\n",
      "            (self): XLMRobertaSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): XLMRobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): XLMRobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): XLMRobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): XLMRobertaLMHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (decoder): Linear(in_features=768, out_features=21, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "for params in model.base_model.parameters(): \n",
    "    params.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"uitnlp/visobert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLMRobertaTokenizerFast(name_or_path='uitnlp/visobert', vocab_size=15002, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t15001: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converter(tokens): \n",
    "    converted_tokens = [] \n",
    "\n",
    "    for token in tokens: \n",
    "        converted_tokens.append(LABEL_2_ID[token])\n",
    "\n",
    "    return converted_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5028\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "df_train[\"tokens\"] = df_train[\"tokens\"].apply(func=converter)\n",
    "print(df_train[\"tokens\"].count())\n",
    "\n",
    "df_test[\"tokens\"] = df_test[\"tokens\"].apply(func=converter)\n",
    "print(df_test[\"tokens\"].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "train_tokens = tokenizer(df_train[\"words\"].to_list(), truncation=True, padding=True, return_tensors=\"pt\", is_split_into_words=True)\n",
    "test_tokens = tokenizer(df_test[\"words\"].to_list(), truncation=True, padding=True, return_tensors=\"pt\", is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "tensor([   0, 2155,  543,  171, 1045, 1287,  609, 1159,  798,  673,  236, 4175,\n",
      "        1696,  964, 1270,  694, 1045, 6296,  317, 4963,  527, 1299, 1134,  104,\n",
      "        1718,  509,  980,  140,    2,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1])\n",
      "[20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 12, 13, 13, 20]\n",
      "[20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 12, 13, 13, 20]\n"
     ]
    }
   ],
   "source": [
    "word_ids = train_tokens.word_ids(batch_index=0)\n",
    "print(word_ids)\n",
    "print(train_tokens[\"input_ids\"][0])\n",
    "reconstruct = [] \n",
    "\n",
    "for word_idx in word_ids: \n",
    "    if word_idx != None: \n",
    "        reconstruct.append(df_train[\"tokens\"].loc[0][word_idx])\n",
    "\n",
    "print(reconstruct)\n",
    "print(df_train[\"tokens\"].loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_tokens(tokens, df_type, label_all_tokens=True): \n",
    "    labels = [] \n",
    "    for i, label in enumerate(df_type[\"tokens\"]):\n",
    "        word_ids = tokens.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokens[\"labels\"] = labels\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = align_tokens(train_tokens, df_type=df_train)\n",
    "test_tokens = align_tokens(test_tokens, df_type=df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5028\n"
     ]
    }
   ],
   "source": [
    "labels = train_tokens[\"labels\"]\n",
    "\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisoDataset(Dataset): \n",
    "    def __init__(self, tokens: pd.Series, labels: list): \n",
    "        self.labels = labels\n",
    "        self.input_ids = tokens[\"input_ids\"]\n",
    "        self.attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "        self.length = len(self.input_ids)\n",
    "\n",
    "    def __len__(self): \n",
    "        return self.length \n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        input_id = self.input_ids[idx]\n",
    "        attention_mask = self.attention_mask[idx]\n",
    "\n",
    "        return {\n",
    "            \"labels\": label, \n",
    "            \"input_ids\": input_id, \n",
    "            \"attention_mask\": attention_mask,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VisoDataset(tokens=train_tokens, labels=train_tokens[\"labels\"])\n",
    "test_dataset = VisoDataset(tokens=test_tokens, labels=test_tokens[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([241])\n",
      "torch.Size([241])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0][\"labels\"].shape) \n",
    "print(train_dataset[1][\"labels\"].shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=128, shuffle=True, num_workers=16, pin_memory=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=128, shuffle=True, num_workers=16, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = nn.CrossEntropyLoss(reduction=\"mean\", ignore_index=-100)\n",
    "\n",
    "output_metrics = {\n",
    "    \"train\": {\n",
    "        \"loss\": [], \n",
    "        \"accuracy\": [], \n",
    "        \"precision\": [], \n",
    "        \"recall\": [], \n",
    "        \"f1\": [], \n",
    "    }, \n",
    "    \"eval\": {\n",
    "        \"loss\": [], \n",
    "        \"accuracy\": [], \n",
    "        \"precision\": [], \n",
    "        \"recall\": [], \n",
    "        \"f1\": [], \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batches epoch 0: 100%|██████████| 40/40 [01:17<00:00,  1.95s/it]\n",
      "Testing batches epoch 0: 100%|██████████| 24/24 [00:35<00:00,  1.48s/it]\n",
      "Training batches epoch 1: 100%|██████████| 40/40 [01:18<00:00,  1.97s/it]\n",
      "Testing batches epoch 1: 100%|██████████| 24/24 [00:35<00:00,  1.48s/it]\n",
      "Training batches epoch 2: 100%|██████████| 40/40 [01:19<00:00,  1.98s/it]\n",
      "Testing batches epoch 2: 100%|██████████| 24/24 [00:35<00:00,  1.49s/it]\n",
      "Training batches epoch 3: 100%|██████████| 40/40 [01:19<00:00,  1.99s/it]\n",
      "Testing batches epoch 3: 100%|██████████| 24/24 [00:35<00:00,  1.49s/it]\n",
      "Training batches epoch 4: 100%|██████████| 40/40 [01:19<00:00,  2.00s/it]\n",
      "Testing batches epoch 4: 100%|██████████| 24/24 [00:35<00:00,  1.49s/it]\n",
      "Training batches epoch 5: 100%|██████████| 40/40 [01:19<00:00,  1.99s/it]\n",
      "Testing batches epoch 5: 100%|██████████| 24/24 [00:35<00:00,  1.49s/it]\n",
      "Training batches epoch 6: 100%|██████████| 40/40 [01:19<00:00,  1.99s/it]\n",
      "Testing batches epoch 6: 100%|██████████| 24/24 [00:35<00:00,  1.49s/it]\n",
      "Training batches epoch 7: 100%|██████████| 40/40 [01:19<00:00,  2.00s/it]\n",
      "Testing batches epoch 7: 100%|██████████| 24/24 [00:35<00:00,  1.50s/it]\n",
      "Training batches epoch 8: 100%|██████████| 40/40 [01:19<00:00,  2.00s/it]\n",
      "Testing batches epoch 8: 100%|██████████| 24/24 [00:35<00:00,  1.49s/it]\n",
      "Training batches epoch 9: 100%|██████████| 40/40 [01:19<00:00,  2.00s/it]\n",
      "Testing batches epoch 9: 100%|██████████| 24/24 [00:35<00:00,  1.50s/it]\n",
      "Training batches epoch 10: 100%|██████████| 40/40 [01:19<00:00,  2.00s/it]\n",
      "Testing batches epoch 10: 100%|██████████| 24/24 [00:35<00:00,  1.50s/it]\n",
      "Training batches epoch 11: 100%|██████████| 40/40 [01:20<00:00,  2.00s/it]\n",
      "Testing batches epoch 11: 100%|██████████| 24/24 [00:35<00:00,  1.50s/it]\n",
      "Training batches epoch 12: 100%|██████████| 40/40 [01:20<00:00,  2.00s/it]\n",
      "Testing batches epoch 12: 100%|██████████| 24/24 [00:35<00:00,  1.50s/it]\n",
      "Training batches epoch 13: 100%|██████████| 40/40 [01:20<00:00,  2.01s/it]\n",
      "Testing batches epoch 13: 100%|██████████| 24/24 [00:35<00:00,  1.50s/it]\n",
      "Training batches epoch 14: 100%|██████████| 40/40 [01:20<00:00,  2.00s/it]\n",
      "Testing batches epoch 14: 100%|██████████| 24/24 [00:35<00:00,  1.50s/it]\n",
      "Epochs: 100%|██████████| 15/15 [28:52<00:00, 115.53s/it]\n"
     ]
    }
   ],
   "source": [
    "import torcheval.metrics\n",
    "\n",
    "\n",
    "epochs = 15\n",
    "\n",
    "for i in tqdm(range(epochs), desc=\"Epochs\", total=epochs): \n",
    "\n",
    "    # training loop\n",
    "    epoch_loss = 0.0 \n",
    "    epoch_accuracy = 0.0\n",
    "    epoch_precision = 0.0\n",
    "    epoch_recall = 0.0\n",
    "    epoch_f1 = 0.0\n",
    "    total_train_steps = len(train_loader)\n",
    "    total_test_steps = len(test_loader)\n",
    "    model = model.train()\n",
    "\n",
    "    for input_dict in tqdm(train_loader, desc=f\"Training batches epoch {i}\", total=total_train_steps):\n",
    "        input_ids = input_dict[\"input_ids\"].to(device)\n",
    "        labels = input_dict[\"labels\"].to(device)\n",
    "        attention_mask = input_dict[\"attention_mask\"].to(device)\n",
    "\n",
    "\n",
    "        logits = model(input_ids, attention_mask)[\"logits\"]\n",
    "\n",
    "        labels = labels.view(-1)\n",
    "        logits = logits.view(-1, 21) \n",
    "\n",
    "        # compute loss \n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        # backpropagation \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update running loss\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # compute metrics\n",
    "        with torch.no_grad(): \n",
    "            epoch_accuracy += torcheval.metrics.functional.multiclass_accuracy(input=logits, target=labels, average=\"micro\").item()\n",
    "            epoch_f1 += torcheval.metrics.functional.multiclass_f1_score(input=logits, target=labels, average=\"micro\").item()\n",
    "            epoch_precision += torcheval.metrics.functional.multiclass_precision(input=logits, target=labels, average=\"micro\").item()\n",
    "            epoch_recall += torcheval.metrics.functional.multiclass_recall(input=logits, target=labels, average=\"micro\").item()\n",
    "\n",
    "    output_metrics[\"train\"][\"loss\"].append(epoch_loss / total_train_steps)\n",
    "    output_metrics[\"train\"][\"accuracy\"].append(epoch_accuracy / total_train_steps)\n",
    "    output_metrics[\"train\"][\"precision\"].append(epoch_precision / total_train_steps)\n",
    "    output_metrics[\"train\"][\"recall\"].append(epoch_recall / total_train_steps)\n",
    "    output_metrics[\"train\"][\"f1\"].append(epoch_f1 / total_train_steps)\n",
    "\n",
    "    # evaluation loop\n",
    "    model = model.eval()\n",
    "    epoch_loss = 0.0 \n",
    "    epoch_accuracy = 0.0\n",
    "    epoch_precision = 0.0\n",
    "    epoch_recall = 0.0\n",
    "    epoch_f1 = 0.0\n",
    "    total_train_steps = len(train_loader)\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        for input_dict in tqdm(test_loader, desc=f\"Testing batches epoch {i}\", total=total_test_steps): \n",
    "            input_ids = input_dict[\"input_ids\"].to(device)\n",
    "            labels = input_dict[\"labels\"].to(device)\n",
    "            attention_mask = input_dict[\"attention_mask\"].to(device)\n",
    "\n",
    "\n",
    "            logits = model(input_ids, attention_mask)[\"logits\"]\n",
    "\n",
    "            labels = labels.view(-1)\n",
    "            logits = logits.view(-1, 21) \n",
    "\n",
    "            # compute loss \n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            # update running loss\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # compute metrics\n",
    "            epoch_accuracy += torcheval.metrics.functional.multiclass_accuracy(input=logits, target=labels, average=\"micro\").item()\n",
    "            epoch_f1 += torcheval.metrics.functional.multiclass_f1_score(input=logits, target=labels, average=\"micro\").item()\n",
    "            epoch_precision += torcheval.metrics.functional.multiclass_precision(input=logits, target=labels, average=\"micro\").item()\n",
    "            epoch_recall += torcheval.metrics.functional.multiclass_recall(input=logits, target=labels, average=\"micro\").item()\n",
    "\n",
    "    output_metrics[\"eval\"][\"loss\"].append(epoch_loss / total_train_steps)\n",
    "    output_metrics[\"eval\"][\"accuracy\"].append(epoch_accuracy / total_train_steps)\n",
    "    output_metrics[\"eval\"][\"precision\"].append(epoch_precision / total_train_steps)\n",
    "    output_metrics[\"eval\"][\"recall\"].append(epoch_recall / total_train_steps)\n",
    "    output_metrics[\"eval\"][\"f1\"].append(epoch_f1 / total_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': {'loss': [1.9219258576631546,\n",
       "   1.158964093029499,\n",
       "   0.9161969318985939,\n",
       "   0.7547978684306145,\n",
       "   0.6448783621191978,\n",
       "   0.5668512493371963,\n",
       "   0.505055095255375,\n",
       "   0.4602366164326668,\n",
       "   0.4201956197619438,\n",
       "   0.3924055278301239,\n",
       "   0.3672646075487137,\n",
       "   0.34822957664728166,\n",
       "   0.33171152547001836,\n",
       "   0.3159734770655632,\n",
       "   0.3045918859541416],\n",
       "  'accuracy': [0.07503269067965448,\n",
       "   0.10744727402925491,\n",
       "   0.11258221920579672,\n",
       "   0.11644326560199261,\n",
       "   0.11989450752735138,\n",
       "   0.12262581996619701,\n",
       "   0.12493427190929651,\n",
       "   0.12646867651492358,\n",
       "   0.12808709796518086,\n",
       "   0.12951182536780834,\n",
       "   0.13060643877834083,\n",
       "   0.13110440000891685,\n",
       "   0.13149376418441533,\n",
       "   0.13233786821365356,\n",
       "   0.13318116050213574],\n",
       "  'precision': [0.07503268732689321,\n",
       "   0.10744726825505495,\n",
       "   0.11258221305906772,\n",
       "   0.11644325964152813,\n",
       "   0.11989450100809336,\n",
       "   0.12262581437826156,\n",
       "   0.12493426594883203,\n",
       "   0.1264686705544591,\n",
       "   0.12808708865195512,\n",
       "   0.1295118175446987,\n",
       "   0.1306064326316118,\n",
       "   0.13110439386218786,\n",
       "   0.13149375822395087,\n",
       "   0.13233786094933747,\n",
       "   0.13318115398287772],\n",
       "  'recall': [0.07503268732689321,\n",
       "   0.10744726825505495,\n",
       "   0.11258221305906772,\n",
       "   0.11644325964152813,\n",
       "   0.11989450100809336,\n",
       "   0.12262581437826156,\n",
       "   0.12493426594883203,\n",
       "   0.1264686705544591,\n",
       "   0.12808708865195512,\n",
       "   0.1295118175446987,\n",
       "   0.1306064326316118,\n",
       "   0.13110439386218786,\n",
       "   0.13149375822395087,\n",
       "   0.13233786094933747,\n",
       "   0.13318115398287772],\n",
       "  'f1': [0.07503268709406256,\n",
       "   0.10744726862758398,\n",
       "   0.11258221380412578,\n",
       "   0.11644326020032167,\n",
       "   0.11989450119435788,\n",
       "   0.12262581456452608,\n",
       "   0.12493426594883203,\n",
       "   0.1264686705544591,\n",
       "   0.12808708865195512,\n",
       "   0.1295118175446987,\n",
       "   0.1306064326316118,\n",
       "   0.13110439386218786,\n",
       "   0.13149375822395087,\n",
       "   0.13233786094933747,\n",
       "   0.13318115398287772]},\n",
       " 'eval': {'loss': [0.9150156587362289,\n",
       "   0.7006440490484238,\n",
       "   0.5725183755159378,\n",
       "   0.4845288097858429,\n",
       "   0.42202694714069366,\n",
       "   0.37376110404729845,\n",
       "   0.33908817917108536,\n",
       "   0.31357299014925955,\n",
       "   0.2916018754243851,\n",
       "   0.27348856553435325,\n",
       "   0.2593699745833874,\n",
       "   0.24590682983398438,\n",
       "   0.2357752040028572,\n",
       "   0.22686398699879645,\n",
       "   0.2189921922981739],\n",
       "  'accuracy': [0.07756454329937697,\n",
       "   0.08361225463449955,\n",
       "   0.08776563443243504,\n",
       "   0.09108281657099723,\n",
       "   0.0943155787885189,\n",
       "   0.09676438719034194,\n",
       "   0.09859181232750416,\n",
       "   0.10023801736533641,\n",
       "   0.10123294405639172,\n",
       "   0.10237435474991799,\n",
       "   0.102679992467165,\n",
       "   0.10377229116857052,\n",
       "   0.10380289778113365,\n",
       "   0.10451752468943595,\n",
       "   0.10470244623720645],\n",
       "  'precision': [0.07756454404443502,\n",
       "   0.08361225612461567,\n",
       "   0.08776563704013825,\n",
       "   0.09108281694352627,\n",
       "   0.09431558027863503,\n",
       "   0.09676439017057419,\n",
       "   0.09859181381762028,\n",
       "   0.10023801922798156,\n",
       "   0.10123294442892075,\n",
       "   0.10237435698509216,\n",
       "   0.10267999432981015,\n",
       "   0.10377229265868664,\n",
       "   0.1038028996437788,\n",
       "   0.10451752729713917,\n",
       "   0.10470244698226452],\n",
       "  'recall': [0.07756454404443502,\n",
       "   0.08361225612461567,\n",
       "   0.08776563704013825,\n",
       "   0.09108281694352627,\n",
       "   0.09431558027863503,\n",
       "   0.09676439017057419,\n",
       "   0.09859181381762028,\n",
       "   0.10023801922798156,\n",
       "   0.10123294442892075,\n",
       "   0.10237435698509216,\n",
       "   0.10267999432981015,\n",
       "   0.10377229265868664,\n",
       "   0.1038028996437788,\n",
       "   0.10451752729713917,\n",
       "   0.10470244698226452],\n",
       "  'f1': [0.07756454404443502,\n",
       "   0.08361225612461567,\n",
       "   0.08776563704013825,\n",
       "   0.09108281694352627,\n",
       "   0.09431558027863503,\n",
       "   0.09676439017057419,\n",
       "   0.09859181381762028,\n",
       "   0.10023801922798156,\n",
       "   0.10123294442892075,\n",
       "   0.1023743573576212,\n",
       "   0.10267999432981015,\n",
       "   0.10377229265868664,\n",
       "   0.10380290038883686,\n",
       "   0.1045175265520811,\n",
       "   0.10470244735479355]}}"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Từ', '24', '-', '7', 'đến', '31', '-', '7', ',', 'bệnh', 'nhân', 'được', 'mẹ', 'là', 'bà', 'H.T.P', '(', '47', 'tuổi', ')', 'đón', 'về', 'nhà', 'ở', 'phường', 'Phước', 'Hoà', '(', 'bằng', 'xe', 'máy', ')', ',', 'không', 'đi', 'đâu', 'chỉ', 'ra', 'Tạp', 'hoá', 'Phượng', ',', 'chợ', 'Vườn', 'Lài', ',', 'phường', 'An', 'Sơn', 'cùng', 'mẹ', 'bán', 'tạp', 'hoá', 'ở', 'đây', '.']\n"
     ]
    }
   ],
   "source": [
    "test = df_test[\"words\"].loc[0]\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_token = tokenizer(test, is_split_into_words=True, truncation=True, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,  1617,  4763,   317,   456,   271, 10811,   317,   456,   171,\n",
       "          1045,   565,   111,   285,    29,   532,    42, 14660, 14669, 14660,\n",
       "         14721,   538,  4650,   774,  1773,  2097,   206,   226,   198,  3510,\n",
       "          3475,  4072,   538,   567,   404,   478,  1773,   171,    77,    48,\n",
       "           218,   229,   141,    21,  1070,  1688,  3229,   171,  2470,    89,\n",
       "          3447,    73,    96,   171,  3510,  1043,  1711,   503,   285,   600,\n",
       "          4309,  1688,   198,   336,   140,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 15, 15, 15, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 38, 39, 40, 41, 42, 43, 43, 44, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, None]\n",
      "[20, 18, 19, 19, 20, 18, 19, 19, 20, 20, 20, 20, 20, 20, 20, 2, 2, 2, 2, 2, 20, 4, 20, 20, 20, 20, 20, 20, 10, 11, 11, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 10, 10, 11, 11, 20, 10, 11, 11, 11, 11, 20, 10, 11, 11, 20, 20, 8, 9, 9, 20, 20, 20]\n"
     ]
    }
   ],
   "source": [
    "word_ids = test_token.word_ids()\n",
    "print(word_ids)\n",
    "reconstruct = [] \n",
    "\n",
    "for word_idx in word_ids: \n",
    "    if word_idx != None: \n",
    "        reconstruct.append(df_test[\"tokens\"].loc[0][word_idx])\n",
    "\n",
    "print(reconstruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(test_token[\"input_ids\"].to(device), test_token[\"attention_mask\"].to(device))[\"logits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[20, 20, 18, 19, 19, 20, 18, 19, 19, 20, 20, 20, 20, 20, 20, 20,  2,  2,\n",
      "          2,  2,  2, 20,  4, 20, 20, 20, 20, 20, 20, 10, 11, 11, 20, 20, 20, 20,\n",
      "         20, 20, 20, 20, 20, 20, 20,  2, 20, 11, 11, 20, 10, 11, 13, 11, 11, 20,\n",
      "         10, 11, 11, 20, 20, 20, 20, 20, 20, 20, 20, 20]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "classes = logits.argmax(dim=-1)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = classes.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list = []\n",
    "\n",
    "\n",
    "for token in classes[0]: \n",
    "    token_list.append(ID_2_LABEL[token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NAME', 'B-NAME', 'B-NAME', 'B-NAME', 'B-NAME', 'O', 'B-AGE', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOCATION', 'I-LOCATION', 'I-LOCATION', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NAME', 'O', 'I-LOCATION', 'I-LOCATION', 'O', 'B-LOCATION', 'I-LOCATION', 'I-ORGANIZATION', 'I-LOCATION', 'I-LOCATION', 'O', 'B-LOCATION', 'I-LOCATION', 'I-LOCATION', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[[20 20 18 19 19 20 18 19 19 20 20 20 20 20 20 20  2  2  2  2  2 20  4 20\n",
      "  20 20 20 20 20 10 11 11 20 20 20 20 20 20 20 20 20 20 20  2 20 11 11 20\n",
      "  10 11 13 11 11 20 10 11 11 20 20 20 20 20 20 20 20 20]]\n",
      "[20, 18, 19, 19, 20, 18, 19, 19, 20, 20, 20, 20, 20, 20, 20, 2, 2, 2, 2, 2, 20, 4, 20, 20, 20, 20, 20, 20, 10, 11, 11, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 10, 10, 11, 11, 20, 10, 11, 11, 11, 11, 20, 10, 11, 11, 20, 20, 8, 9, 9, 20, 20, 20]\n",
      "66\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "print(token_list)\n",
    "print(classes)\n",
    "print(reconstruct)\n",
    "\n",
    "print(len(classes[0]))\n",
    "print(len(reconstruct))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
