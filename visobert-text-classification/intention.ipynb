{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dataset import read_intention_data, read_intention_data_multiclass\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_2_ID = {'TRADEMARK': 0, 'INTEREST_RATE': 1, 'ACCOUNT': 2, 'SECURITY': 3, 'CARD': 4, 'SAVING': 5, 'CUSTOMER_SUPPORT': 6,\n",
    "                'PROMOTION': 7, 'MONEY_TRANSFER': 8, 'PAYMENT': 9, 'DISCOUNT': 10, 'LOAN': 11, 'OTHER': 12, 'INTERNET_BANKING': 13}\n",
    "\n",
    "\n",
    "ID_2_LABEL = {0: 'TRADEMARK', 1: 'INTEREST_RATE', 2: 'ACCOUNT', 3: 'SECURITY', 4: 'CARD', 5: 'SAVING', 6: 'CUSTOMER_SUPPORT', 7: 'PROMOTION', 8: 'MONEY_TRANSFER', 9: 'PAYMENT', 10: 'DISCOUNT', 11: 'LOAN', 12: 'OTHER', 13: 'INTERNET_BANKING'}\n",
    "\n",
    "num_classes = len(LABEL_2_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = read_intention_data(\"./data/intention/train.txt\")\n",
    "# df_test = read_sentiment_data(\"./data/sentiment/test.txt\")\n",
    "\n",
    "df_train = pd.DataFrame(data=df_train)\n",
    "df_train = df_train.convert_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class TRADEMARK has 699 instances\n",
      "class INTEREST_RATE has 68 instances\n",
      "class ACCOUNT has 5 instances\n",
      "class SECURITY has 5 instances\n",
      "class CARD has 67 instances\n",
      "class SAVING has 13 instances\n",
      "class CUSTOMER_SUPPORT has 784 instances\n",
      "class PROMOTION has 53 instances\n",
      "class MONEY_TRANSFER has 36 instances\n",
      "class PAYMENT has 15 instances\n",
      "class DISCOUNT has 42 instances\n",
      "class LOAN has 74 instances\n",
      "class OTHER has 69 instances\n",
      "class INTERNET_BANKING has 79 instances\n",
      "Number of quotes: 1977\n"
     ]
    }
   ],
   "source": [
    "# count of each class \n",
    "label = df_train[\"label\"]\n",
    "\n",
    "\n",
    "def get_class_count_multiclass(label: pd.Series, classname): \n",
    "    label = label.apply(func=lambda x: x[classname] == 1)\n",
    "    pos = label[label == True].count()\n",
    "\n",
    "    return pos \n",
    "\n",
    "for i in range(num_classes): \n",
    "    print(f\"class {ID_2_LABEL[i]} has {get_class_count_multiclass(label, classname=i)} instances\")\n",
    "\n",
    "print(f\"Number of quotes: {df_train['label'].count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"5CD-AI/Vietnamese-Sentiment-visobert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaForSequenceClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(15004, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): XLMRobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = base_model.base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLMRobertaModel(\n",
      "  (embeddings): XLMRobertaEmbeddings(\n",
      "    (word_embeddings): Embedding(15004, 768, padding_idx=1)\n",
      "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): XLMRobertaEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x XLMRobertaLayer(\n",
      "        (attention): XLMRobertaAttention(\n",
      "          (self): XLMRobertaSdpaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): XLMRobertaSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): XLMRobertaIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): XLMRobertaOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelVisobert(nn.Module): \n",
    "    def __init__(self, base_model, encoder_outc, num_classes): \n",
    "        super().__init__()\n",
    "        \n",
    "        self.base_model = base_model \n",
    "        self.classifier = nn.ModuleList([\n",
    "            nn.Linear(in_features=encoder_outc, out_features=encoder_outc, bias=True), \n",
    "            nn.Dropout(p=0.1, inplace=False), \n",
    "            nn.Linear(in_features=encoder_outc, out_features=num_classes, bias=True)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, attention_mask): \n",
    "        x = self.base_model(x, attention_mask) \n",
    "        x = x.last_hidden_state\n",
    "        x = x[:, 0, :]\n",
    "        for module in self.classifier: \n",
    "            x = module(x) \n",
    "\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for params in base_model.parameters(): \n",
    "    params.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiLabelVisobert(base_model=base_model, encoder_outc=768, num_classes=num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLabelVisobert(\n",
       "  (base_model): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(15004, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): ModuleList(\n",
       "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (1): Dropout(p=0.1, inplace=False)\n",
       "    (2): Linear(in_features=768, out_features=14, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"5CD-AI/Vietnamese-Sentiment-visobert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "In this dataset, 1 quote can be associated with multiple intentions. This means that we can train this model as a multi-label classifier. We can also just train this as a pure multi-class classifier because the data for some intentions are very rare, causing them to not be sufficiently trained.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-label train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = tokenizer(df_train[\"value\"].to_list(), truncation=True, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisoDataset(Dataset): \n",
    "    def __init__(self, tokens: pd.Series, label: pd.Series): \n",
    "        self.label = label\n",
    "        self.input_ids = tokens[\"input_ids\"]\n",
    "        self.attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "        self.length = len(self.input_ids)\n",
    "\n",
    "    def __len__(self): \n",
    "        return self.length \n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        label = torch.tensor(self.label.loc[idx], dtype=torch.float32) \n",
    "        input_id = self.input_ids[idx]\n",
    "        attention_mask = self.attention_mask[idx]\n",
    "\n",
    "        return {\n",
    "            \"labels\": label, \n",
    "            \"input_ids\": input_id, \n",
    "            \"attention_mask\": attention_mask,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VisoDataset(tokens=train_tokens, label=df_train[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'input_ids': tensor([   0, 2615,  749,  970,   50, 2786,   17, 2321,    2,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=4, shuffle=True, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyle/miniconda3/envs/dl/lib/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "\n",
    "loss_history = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 10/10 [05:33<00:00, 33.31s/it]\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for i in tqdm(range(epochs), desc=\"Epochs\", total=epochs): \n",
    "    epoch_loss = 0.0 \n",
    "    total_steps = len(train_loader)\n",
    "\n",
    "    for input_dict in train_loader: \n",
    "        input_ids = input_dict[\"input_ids\"].to(device)\n",
    "        labels = input_dict[\"labels\"].to(device)\n",
    "        attention_mask = input_dict[\"attention_mask\"].to(device)\n",
    "\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    loss_history.append(epoch_loss / total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.26008071052004594,\n",
       " 0.15660022032456566,\n",
       " 0.16292103533727126,\n",
       " 0.1848858987303413,\n",
       " 0.19245248282113056,\n",
       " 0.19669333255856833,\n",
       " 0.21083191876479593,\n",
       " 0.21409094560291017,\n",
       " 0.24566511762552384,\n",
       " 0.19201297443849746]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class train \n",
    "\n",
    "For multi-class train, we need to normalize the dataset so that there are no class imbalances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_2_ID_MULTICLASS = {\"TRADEMARK\": 0, \"CUSTOMER_SUPPORT\": 1, \"OTHER\": 2}\n",
    "ID_2_LABEL_MULTICLASS = {0: \"TRADEMARK\", 1: \"CUSTOMER_SUPPORT\", 2: \"OTHER\"}\n",
    "\n",
    "num_classes_multiclass = len(LABEL_2_ID_MULTICLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_multiclass = read_intention_data_multiclass(\n",
    "    \"./data/intention/train_multiclass.txt\")\n",
    "\n",
    "df_train_multiclass = pd.DataFrame(df_train_multiclass)\n",
    "df_train_multiclass = df_train_multiclass.convert_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Cần tư vấn mà add  k rep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Hotline khó gọi quá gọi mãi ko thưa máy à</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Mình thấy câu dịch vụ tốt nhất cho kh khó lắm....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Em chọn chuyển tiền trong nước. Chuyển đến số ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Mình xài cái thể VISA của BIDV hạn mức 100tr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>1972</td>\n",
       "      <td>1</td>\n",
       "      <td>Dạ em cảm ơn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>1973</td>\n",
       "      <td>2</td>\n",
       "      <td>Có kinh nghiệm nhưng phải bằng đại học chính q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>1974</td>\n",
       "      <td>1</td>\n",
       "      <td>Vietcombank tks add trước nha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>1975</td>\n",
       "      <td>1</td>\n",
       "      <td>Vietcombank ok tks add</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1976</th>\n",
       "      <td>1976</td>\n",
       "      <td>1</td>\n",
       "      <td>Gọi k được mà tốn tiền như gì ấy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1977 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  label                                              value\n",
       "0        0      1                          Cần tư vấn mà add  k rep\n",
       "\n",
       "1        1      1         Hotline khó gọi quá gọi mãi ko thưa máy à\n",
       "\n",
       "2        2      1  Mình thấy câu dịch vụ tốt nhất cho kh khó lắm....\n",
       "3        3      0  Em chọn chuyển tiền trong nước. Chuyển đến số ...\n",
       "4        4      2       Mình xài cái thể VISA của BIDV hạn mức 100tr\n",
       "...    ...    ...                                                ...\n",
       "1972  1972      1                                      Dạ em cảm ơn\n",
       "\n",
       "1973  1973      2  Có kinh nghiệm nhưng phải bằng đại học chính q...\n",
       "1974  1974      1                     Vietcombank tks add trước nha\n",
       "\n",
       "1975  1975      1                            Vietcombank ok tks add\n",
       "\n",
       "1976  1976      1                  Gọi k được mà tốn tiền như gì ấy\n",
       "\n",
       "\n",
       "[1977 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class TRADEMARK has 699 instances, taking up 35.35660091047041% of the dataset\n",
      "class CUSTOMER_SUPPORT has 774 instances, taking up 39.150227617602425% of the dataset\n",
      "class OTHER has 504 instances, taking up 25.493171471927162% of the dataset\n",
      "Number of quotes: 1977\n"
     ]
    }
   ],
   "source": [
    "label = df_train_multiclass[\"label\"]\n",
    "\n",
    "def get_class_count_multiclass(label: pd.Series, classname): \n",
    "    pos = label[label == classname].count()\n",
    "    return pos \n",
    "\n",
    "for i in range(num_classes_multiclass): \n",
    "    count = get_class_count_multiclass(label, classname=i)\n",
    "    print(f\"class {ID_2_LABEL_MULTICLASS[i]} has {count} instances, taking up {count / label.count() * 100}% of the dataset\")\n",
    "\n",
    "print(f\"Number of quotes: {df_train_multiclass['label'].count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_multiclass = MultiLabelVisobert(base_model=base_model, encoder_outc=768, num_classes=num_classes_multiclass).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens_multiclass = tokenizer(df_train_multiclass[\"value\"].to_list(), truncation=True, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_multiclass = VisoDataset(tokens=train_tokens_multiclass, label=df_train_multiclass[\"label\"])\n",
    "train_loader_multiclass = DataLoader(dataset=train_dataset_multiclass, batch_size=128, shuffle=True, num_workers=16, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_multiclass = AdamW(model_multiclass.parameters(), lr=1e-5)\n",
    "criterion_multiclass = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "loss_history_multiclass = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 10/10 [05:03<00:00, 30.35s/it]\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for i in tqdm(range(epochs), desc=\"Epochs\", total=epochs): \n",
    "    epoch_loss = 0.0 \n",
    "    total_steps = len(train_loader_multiclass)\n",
    "\n",
    "    for input_dict in train_loader_multiclass: \n",
    "        input_ids = input_dict[\"input_ids\"].to(device)\n",
    "        labels = input_dict[\"labels\"].long().to(device)\n",
    "        attention_mask = input_dict[\"attention_mask\"].to(device)\n",
    "\n",
    "        logits = model_multiclass(input_ids, attention_mask)\n",
    "        loss = criterion_multiclass(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer_multiclass.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    loss_history_multiclass.append(epoch_loss / total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8206733800470829,\n",
       " 0.8539123684167862,\n",
       " 0.8558911122381687,\n",
       " 0.7922081016004086,\n",
       " 0.716634813696146,\n",
       " 0.6465386040508747,\n",
       " 0.621187211945653,\n",
       " 0.650752292945981,\n",
       " 0.6999602951109409,\n",
       " 0.7664244472980499]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_history_multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
