{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dataset import read_sentiment_data\n",
    "import torch \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np \n",
    "import os\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_2_LABEL = {\n",
    "    2: \"positive\", \n",
    "    1: \"negative\", \n",
    "    0: \"neutral\", \n",
    "}\n",
    "\n",
    "LABEL_2_ID = {\n",
    "     \"positive\": 2, \n",
    "     \"negative\": 1, \n",
    "     \"neutral\": 0 \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = read_sentiment_data(\"./data/sentiment/train.txt\")\n",
    "df_test = read_sentiment_data(\"./data/sentiment/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(data=df_train)\n",
    "df_train = df_train.convert_dtypes()\n",
    "\n",
    "df_test = pd.DataFrame(data=df_test) \n",
    "df_test = df_test.convert_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id  label                                              value\n",
      "0        0      1                          Cần tư vấn mà add  k rep\n",
      "\n",
      "1        1      1         Hotline khó gọi quá gọi mãi ko thưa máy à\n",
      "\n",
      "2        2      1  Mình thấy câu dịch vụ tốt nhất cho kh khó lắm....\n",
      "3        3      1  Em chọn chuyển tiền trong nước. Chuyển đến số ...\n",
      "4        4      1       Mình xài cái thể VISA của BIDV hạn mức 100tr\n",
      "...    ...    ...                                                ...\n",
      "1972  1972      2                                      Dạ em cảm ơn\n",
      "\n",
      "1973  1973      1  Có kinh nghiệm nhưng phải bằng đại học chính q...\n",
      "1974  1974      2                     Vietcombank tks add trước nha\n",
      "\n",
      "1975  1975      2                            Vietcombank ok tks add\n",
      "\n",
      "1976  1976      1                  Gọi k được mà tốn tiền như gì ấy\n",
      "\n",
      "\n",
      "[1977 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive label count: 1211\n",
      "negative label count: 743\n",
      "neutral label count: 23\n",
      "total: 1977 == 1977\n"
     ]
    }
   ],
   "source": [
    "positives = df_train['label'][df_train['label'] == 2].count()\n",
    "negatives = df_train['label'][df_train['label'] == 1].count()\n",
    "neutrals = df_train['label'][df_train['label'] == 0].count()\n",
    "\n",
    "print(f\"positive label count: {positives}\")\n",
    "print(f\"negative label count: {negatives}\")\n",
    "print(f\"neutral label count: {neutrals}\")\n",
    "\n",
    "print(f\"total: {positives + negatives + neutrals} == {df_train['label'].count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyle/miniconda3/envs/dl/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"5CD-AI/Vietnamese-Sentiment-visobert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLMRobertaForSequenceClassification(\n",
      "  (roberta): XLMRobertaModel(\n",
      "    (embeddings): XLMRobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(15004, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): XLMRobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x XLMRobertaLayer(\n",
      "          (attention): XLMRobertaAttention(\n",
      "            (self): XLMRobertaSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): XLMRobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): XLMRobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): XLMRobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): XLMRobertaClassificationHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"5CD-AI/Vietnamese-Sentiment-visobert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaTokenizerFast(name_or_path='5CD-AI/Vietnamese-Sentiment-visobert', vocab_size=15002, model_max_length=256, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t15001: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = tokenizer.vocab\n",
    "reversed_vocab = {v: k for k, v in vocab.items()} # reverse it so that we can retrieve the text from the token \n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cần tư vấn mà add  k rep\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.loc[0].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tokenizer.encode(text=df_train.loc[0].value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 0 -> <s>\n",
      "Token: 2615 -> ▁Cần\n",
      "Token: 749 -> ▁tư\n",
      "Token: 970 -> ▁vấn\n",
      "Token: 50 -> ▁mà\n",
      "Token: 2786 -> ▁add\n",
      "Token: 17 -> ▁k\n",
      "Token: 2321 -> ▁rep\n",
      "Token: 2 -> </s>\n"
     ]
    }
   ],
   "source": [
    "for id in t: \n",
    "    print(f\"Token: {id} -> {reversed_vocab[id]}\") # ignores new line character, a property of sentencepiece "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_long = tokenizer.encode(text=df_train.loc[64].value)\n",
    "\n",
    "for id in t_long: \n",
    "    print(f\"Token: {id} -> {reversed_vocab[id]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cần tư vấn mà add k rep\n"
     ]
    }
   ],
   "source": [
    "s = tokenizer.decode(t, skip_special_tokens=True) \n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = df_train.loc[0]\n",
    "# label = input.label \n",
    "# input = tokenizer.encode(text=input.value) \n",
    "\n",
    "# input = torch.tensor(input, dtype=torch.int32) \n",
    "# input = input.reshape(1, -1)\n",
    "# input = input.to(\"cuda\")\n",
    "# print(input.shape) # (batch size, token length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding = model.get_input_embeddings()\n",
    "# embedding_vector = embedding(input)\n",
    "# print(embedding_vector.shape) # (batch size, token length, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = model(input, labels=torch.tensor([1]).unsqueeze(0)) # the labels of the input) \n",
    "# loss = output.loss\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = torch.argmax(output.logits) \n",
    "# print(res)\n",
    "# print(f\"Prediction label: {ID_2_LABEL[res.item()]}\")\n",
    "# print(f\"Real label: {ID_2_LABEL[label]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_texts = df_train[\"value\"][:500]\n",
    "tokenize_texts = tokenize_texts.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_texts = tokenizer(tokenize_texts, truncation=True, padding=True, return_tensors=\"pt\").to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize_texts[\"input_ids\"]\n",
    "attention_mask = tokenize_texts[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs = model(tokens, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = tokenizer(df_train[\"value\"].to_list(), truncation=True, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisoDataset(Dataset): \n",
    "    def __init__(self, tokens: pd.Series, label: pd.Series): \n",
    "        self.label = label\n",
    "        self.input_ids = tokens[\"input_ids\"]\n",
    "        self.attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "        self.length = len(self.input_ids)\n",
    "\n",
    "    def __len__(self): \n",
    "        return self.length \n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        label = self.label.loc[idx]\n",
    "        input_id = self.input_ids[idx]\n",
    "        attention_mask = self.attention_mask[idx]\n",
    "\n",
    "        return {\n",
    "            \"labels\": label, \n",
    "            \"input_ids\": input_id, \n",
    "            \"attention_mask\": attention_mask,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VisoDataset(tokens=train_tokens, label=df_train[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for params in model.base_model.parameters(): \n",
    "    params.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyle/miniconda3/envs/dl/lib/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from tqdm import tqdm \n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train step:  16%|█▌        | 5/31 [00:05<00:30,  1.17s/it]\n",
      "Epoch:   0%|          | 0/6 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     16\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 18\u001b[0m     losses \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m loss_history\u001b[38;5;241m.\u001b[39mappend(losses \u001b[38;5;241m/\u001b[39m total_steps) \n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 6 \n",
    "\n",
    "for i in tqdm(range(epochs), desc=\"Epoch\", total=epochs): \n",
    "    losses = 0.0\n",
    "    total_steps = len(train_loader)\n",
    "    for input_dict in tqdm(train_loader, desc=\"Train step\", total=total_steps): \n",
    "        input_ids = input_dict[\"input_ids\"].to(device)\n",
    "        labels = input_dict[\"labels\"].to(device)\n",
    "        attention_mask = input_dict[\"attention_mask\"].to(device) \n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        print(outputs.logits.shape)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses += loss.item()\n",
    "\n",
    "    loss_history.append(losses / total_steps) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.856175491886754,\n",
       " 3.0950295540594284,\n",
       " 1.4152748738565752,\n",
       " 0.5592228068459418,\n",
       " 0.4253804289525555,\n",
       " 0.4852882090114778]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history, np.arange(1, 7))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
